# 第九章－推荐系统

&ensp;&ensp;&ensp;&ensp;如果有数据，那数据说了算。如果没有，那么我说了算。

<p align="right">－前 Netscape CEO Jim Barksdale</p>

## 推荐系统是什么？

&ensp;&ensp;&ensp;&ensp;推荐系统也被称为，推荐引擎。它是一种信息系统，主要用来将物品或者动作显示或者推荐给用户。推荐物品通常由少量的物品－电影，书籍，等等，比如：社交网络中的关注其他用户的推荐。从大的集合中根据一些条件，比如，用户之前表现出来的喜好，来选择出一个小的子集，就是一种很典型的做法。其他可能的条件包括：年龄、性别、和位置。

**以下是推荐系统中常用的几种方法:**

* **内容过滤**

&ensp;&ensp;&ensp;&ensp;内容过滤通过收集辅助信息（比如，用户的人口统计数据，音乐流派，关键词，问卷答案）来为每个物品或者用户来生成一个画像。基于用户画像来匹配物品。例如：Pandora的Music Genome项目。

* **协同过滤**

&ensp;&ensp;&ensp;&ensp;协同过滤是基于用户的过去行为。每个用户的排名，或者浏览记录可以让推荐系统在拥有相同行为用户和相同用户之间的物品兴趣之间建立联系。比如：Netflix。

&ensp;&ensp;&ensp;&ensp;协同过滤，得益于它天然的领域开放性，成为了这几种方法中的可能最流行的方法。在协同过滤系统还可以通过基于邻近的方法（基于用户与用户的距离，或者物品与物品的距离）、因子、降维模型－可以用来自动发现用户或者物品的少量描述因子，来进一步划分。低阶矩阵因子分解是降维模型的最著名的方法，同时它也是推荐系统内部最灵活和成功的方法之一。矩阵分解有很多种变体，包括，可能性和贝叶斯版本。另一种目前最先进的方法是一种深度学习神经网络，受限波尔兹曼机。

## 推荐系统有什么用途？

&ensp;&ensp;&ensp;&ensp;推荐系统的应用十分广泛，它可以应用到任何面向海量用户的多种类商品种销售的商业中。零售商，比如：Amazon，Netflix 以及 Target ，网络电影和音乐，比如：Netflix 和 last.fm , 以及社交网站，比如： Facebook 和 Twitter, 都采用了推荐系统。推荐系统也被应用到杂货店，比如：Tesco。

&ensp;&ensp;&ensp;&ensp;除了上面提及的，零售，媒体，以及社交网站的例子外，推荐系统也被，诸如：Yahoo!，Google，这样的通用网站用基于历史浏览和其他信息来推送最好的广告。推荐系统另外一个应用是在市场决策方面的类似下一个最佳购物建议的应用，例如：Schwan 食品公司就利用推荐系统来提高他们冷冻食品的销量。

![&#x56FE; 9.1  &#x77E9;&#x9635;&#x56E0;&#x5B50;&#x5206;&#x89E3;](../../.gitbook/assets/image.png)

## 推荐系统如何工作？

&ensp;&ensp;&ensp;&ensp;我们将会对大多数的推荐系统的底层的基本数学原理进行讲解。首先，我们假设，有一个 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/366cb5a3a2d3ed5d07afd7e548cf1227.svg?invert_in_darkmode&sanitize=true" align=middle width=20.979918299999998pt height=13.698590399999999pt/></p> 行 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d29ee163d3a1cd31b107fe113effe9c0.svg?invert_in_darkmode&sanitize=true" align=middle width=17.8586661pt height=13.698590399999999pt/></p> 列的排名矩阵 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c1809668d64eff2bbf358a60787e123b.svg?invert_in_darkmode&sanitize=true" align=middle width=14.90868885pt height=11.232861749999998pt/></p> ，其中， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/366cb5a3a2d3ed5d07afd7e548cf1227.svg?invert_in_darkmode&sanitize=true" align=middle width=20.979918299999998pt height=13.698590399999999pt/></p> 代表用户的数量， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d29ee163d3a1cd31b107fe113effe9c0.svg?invert_in_darkmode&sanitize=true" align=middle width=17.8586661pt height=13.698590399999999pt/></p> 代表要被排名的物品的数量。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c1809668d64eff2bbf358a60787e123b.svg?invert_in_darkmode&sanitize=true" align=middle width=14.90868885pt height=11.232861749999998pt/></p> 中的元素<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9c9a444254eb9c010e73403922cd6dab.svg?invert_in_darkmode&sanitize=true" align=middle width=21.818039099999996pt height=9.54335085pt/></p> 代表给用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 推荐的物品 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6ac91b4e7dd35551c6ea477deba5f82d.svg?invert_in_darkmode&sanitize=true" align=middle width=5.6632257pt height=10.84150485pt/></p> 的排名。我们往往只知道矩阵 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c1809668d64eff2bbf358a60787e123b.svg?invert_in_darkmode&sanitize=true" align=middle width=14.90868885pt height=11.232861749999998pt/></p> 中的少量元素。假设， 已知的排名以类似： <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6b7361e1f555a5cc1fb9ed7518c0122d.svg?invert_in_darkmode&sanitize=true" align=middle width=209.00884125pt height=17.031940199999998pt/></p>的三元组， 如图9.1所示的方式存储。我们假设已有的排名有 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/b49da7325822089835b531a5fce8b94e.svg?invert_in_darkmode&sanitize=true" align=middle width=9.866876249999999pt height=7.0776222pt/></p> 个。

### 基本模型

&ensp;&ensp;&ensp;&ensp;推荐系统的基本模型可以定义为：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/83e099c8bc6dfea600c8d501300cc704.svg?invert_in_darkmode&sanitize=true" align=middle width=126.52376384999998pt height=13.881256950000001pt/></p>

其中， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a09f8f1b8579bc6f1e79f3e4268bc3c2.svg?invert_in_darkmode&sanitize=true" align=middle width=13.607343749999998pt height=13.881256950000001pt/></p>， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/25f651504db75e0ef71046a88719b574.svg?invert_in_darkmode&sanitize=true" align=middle width=14.82694785pt height=13.881256950000001pt/></p> ， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/f0f68b58c891ba6dde41b41ac8c3b570.svg?invert_in_darkmode&sanitize=true" align=middle width=11.70569565pt height=13.881256950000001pt/></p>  分别为常量， 用户的偏差，物品的偏差。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a09f8f1b8579bc6f1e79f3e4268bc3c2.svg?invert_in_darkmode&sanitize=true" align=middle width=13.607343749999998pt height=13.881256950000001pt/></p> 对应全局平均排名， 而<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/25f651504db75e0ef71046a88719b574.svg?invert_in_darkmode&sanitize=true" align=middle width=14.82694785pt height=13.881256950000001pt/></p> 代表用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 从 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a09f8f1b8579bc6f1e79f3e4268bc3c2.svg?invert_in_darkmode&sanitize=true" align=middle width=13.607343749999998pt height=13.881256950000001pt/></p> 开始的平均偏差总数， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/f0f68b58c891ba6dde41b41ac8c3b570.svg?invert_in_darkmode&sanitize=true" align=middle width=11.70569565pt height=13.881256950000001pt/></p> 代表物品 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6ac91b4e7dd35551c6ea477deba5f82d.svg?invert_in_darkmode&sanitize=true" align=middle width=5.6632257pt height=10.84150485pt/></p> 从 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a09f8f1b8579bc6f1e79f3e4268bc3c2.svg?invert_in_darkmode&sanitize=true" align=middle width=13.607343749999998pt height=13.881256950000001pt/></p> 开始的平均偏差总数。模型的目标是对集合 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a9bb7fa69ed54c7064249aa39a7009e2.svg?invert_in_darkmode&sanitize=true" align=middle width=14.06623185pt height=11.232861749999998pt/></p> 中所有的 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6ac91b4e7dd35551c6ea477deba5f82d.svg?invert_in_darkmode&sanitize=true" align=middle width=5.6632257pt height=10.84150485pt/></p> ，估计 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a09f8f1b8579bc6f1e79f3e4268bc3c2.svg?invert_in_darkmode&sanitize=true" align=middle width=13.607343749999998pt height=13.881256950000001pt/></p>, <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/25f651504db75e0ef71046a88719b574.svg?invert_in_darkmode&sanitize=true" align=middle width=14.82694785pt height=13.881256950000001pt/></p>和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/f0f68b58c891ba6dde41b41ac8c3b570.svg?invert_in_darkmode&sanitize=true" align=middle width=11.70569565pt height=13.881256950000001pt/></p> 。

&ensp;&ensp;&ensp;&ensp;偏差可以通过解决最小二乘法优化问题来估计。

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/39c85a851c94206826e617c9efc37946.svg?invert_in_darkmode&sanitize=true" align=middle width=385.47103979999997pt height=40.548151049999994pt/></p>

其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a09f8f1b8579bc6f1e79f3e4268bc3c2.svg?invert_in_darkmode&sanitize=true" align=middle width=13.607343749999998pt height=13.881256950000001pt/></p> 为所有已知 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9c9a444254eb9c010e73403922cd6dab.svg?invert_in_darkmode&sanitize=true" align=middle width=21.818039099999996pt height=9.54335085pt/></p> 的平均值，第一个子式是模型的观察排名和预测排名的平方误差。  第二个子式是归一化补偿来降低过度拟合时造成的大误差的影响。参数 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/18f8eacfb4280d2c13c04e23edc6650d.svg?invert_in_darkmode&sanitize=true" align=middle width=9.589082249999999pt height=11.4155283pt/></p> 来控制归一化的总值。

![&#x8868; 9.1 &#x5DF2;&#x77E5;&#x6392;&#x540D;&#x7684;&#x6570;&#x636E;&#x96C6;D](../../.gitbook/assets/1947a69e-08a7-4590-a540-e4261a7cce10.jpeg)

## 低阶矩阵因子分解

&ensp;&ensp;&ensp;&ensp;低阶矩阵因子分解提供了一种更加灵活的模型。考虑如下的内积：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/765c2b54493b0d33190acf7097e360c3.svg?invert_in_darkmode&sanitize=true" align=middle width=92.53609365pt height=13.698590399999999pt/></p>

其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9c6e689774de1bd231a07eea782a80e0.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99820135pt height=13.698590399999999pt/></p> 和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a75b3cb7cb1df56887dd548acad7cdd.svg?invert_in_darkmode&sanitize=true" align=middle width=12.067218899999999pt height=9.54335085pt/></p> 都是一个 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/3368f3f6c9ce9a6e66e089fd5133ae91.svg?invert_in_darkmode&sanitize=true" align=middle width=15.137005949999999pt height=11.232861749999998pt/></p> 维矢量。上面的模型也可以改写为矩阵乘积的形式。

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/357c83a8f690c6ffa7cddf71d31ec521.svg?invert_in_darkmode&sanitize=true" align=middle width=60.6220131pt height=11.232861749999998pt/></p>

如图9.1所述，等式左边的因子矩阵 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/e0348ffc63e6ccd14cf6f77395e3853b.svg?invert_in_darkmode&sanitize=true" align=middle width=11.18724255pt height=11.232861749999998pt/></p> 的行由所有用户的 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/578b3af6f46a0f2d98be0807589d5a9b.svg?invert_in_darkmode&sanitize=true" align=middle width=12.677025899999999pt height=13.881256950000001pt/></p> 矢量组成，右边的因子矩阵 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/e5db46b49e39bd8b17d72b912e26bd71.svg?invert_in_darkmode&sanitize=true" align=middle width=12.608473349999999pt height=11.232861749999998pt/></p> 的列由所有物品的 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a75b3cb7cb1df56887dd548acad7cdd.svg?invert_in_darkmode&sanitize=true" align=middle width=12.067218899999999pt height=9.54335085pt/></p> 矢量组成。在这个模型中，正如模型名—低阶矩阵因子分解而言，矩阵 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c1809668d64eff2bbf358a60787e123b.svg?invert_in_darkmode&sanitize=true" align=middle width=14.90868885pt height=11.232861749999998pt/></p> 的阶 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/3368f3f6c9ce9a6e66e089fd5133ae91.svg?invert_in_darkmode&sanitize=true" align=middle width=15.137005949999999pt height=11.232861749999998pt/></p> 远远比 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/366cb5a3a2d3ed5d07afd7e548cf1227.svg?invert_in_darkmode&sanitize=true" align=middle width=20.979918299999998pt height=13.698590399999999pt/></p> 和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d29ee163d3a1cd31b107fe113effe9c0.svg?invert_in_darkmode&sanitize=true" align=middle width=17.8586661pt height=13.698590399999999pt/></p> 要小。

&ensp;&ensp;&ensp;&ensp;与基础模型类似，<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/e05ba506d90c2c6e1189cae3318e39ee.svg?invert_in_darkmode&sanitize=true" align=middle width=11.18724255pt height=11.232861749999998pt/></p>和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/e5db46b49e39bd8b17d72b912e26bd71.svg?invert_in_darkmode&sanitize=true" align=middle width=12.608473349999999pt height=11.232861749999998pt/></p> 的估计可以通过解决如下的优化问题来得到：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/59f25974a5968dcf13af9b5c9c0fc949.svg?invert_in_darkmode&sanitize=true" align=middle width=395.85083669999995pt height=40.548151049999994pt/></p>

同样，该模型通过加入因子范式来避免过度拟合。随机梯度下降和交替最小二乘法是解决这个问题的最常用的方法。

## 随机梯度下降法

&ensp;&ensp;&ensp;&ensp;梯度随机下降法首先设定两个初始值， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/578b3af6f46a0f2d98be0807589d5a9b.svg?invert_in_darkmode&sanitize=true" align=middle width=12.677025899999999pt height=13.881256950000001pt/></p>，<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a75b3cb7cb1df56887dd548acad7cdd.svg?invert_in_darkmode&sanitize=true" align=middle width=12.067218899999999pt height=9.54335085pt/></p> ，然后在此基础上的物体的负梯度方向不断更新相对应的值，如下式：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/442eebff6919f7b6b67ea1ffc4c770a4.svg?invert_in_darkmode&sanitize=true" align=middle width=171.16446435pt height=16.438356pt/></p>

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/7dc73e8165d406198eaabdb0d5fd3e97.svg?invert_in_darkmode&sanitize=true" align=middle width=169.94481405pt height=16.438356pt/></p>

其中 ：

    $$e_{ui} \triangleq x_{ui} - l_u.r_i$$ 为 $$(u,i)$$ 对排名的预测误差。

    $$\eta$$ 为用户定义的学习步长。

    随机梯度下降，顾名思义，它每进行一次排名，就从数据集 $$D$$ 中随机选取一对 $$(u,i)$$ 更新一次。一旦对数据集 $$D$$ 完成一次完全遍历，也就是说一个周期。算法再次以不同的随机顺序对同样的数据集$$D$$。算法不断对数据集重复遍历，直到完全覆盖，这通常需要进行数个遍历周期。

     随机梯度下降法并不需要将所有数据集存入内存中，所以这个方法在数据集 $$D$$很大的时候有很大的优势。

## 交替最小二乘法

&ensp;&ensp;&ensp;&ensp;另一个比较有名的方法是交替最小二乘法，这个算法只有两步。首先，固定右边的 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a75b3cb7cb1df56887dd548acad7cdd.svg?invert_in_darkmode&sanitize=true" align=middle width=12.067218899999999pt height=9.54335085pt/></p> 算出 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/578b3af6f46a0f2d98be0807589d5a9b.svg?invert_in_darkmode&sanitize=true" align=middle width=12.677025899999999pt height=13.881256950000001pt/></p> ，然后通过固定左边 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/578b3af6f46a0f2d98be0807589d5a9b.svg?invert_in_darkmode&sanitize=true" align=middle width=12.677025899999999pt height=13.881256950000001pt/></p>算出 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a75b3cb7cb1df56887dd548acad7cdd.svg?invert_in_darkmode&sanitize=true" align=middle width=12.067218899999999pt height=9.54335085pt/></p>。每个步骤都可以通过最小二乘法来计算。

&ensp;&ensp;&ensp;&ensp;假设， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/b3a261e849a643857ebbe34babe1e68b.svg?invert_in_darkmode&sanitize=true" align=middle width=17.639028pt height=9.54335085pt/></p> 为用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 的排名数， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/31cb95ba4852441821ce8a4fe56120e8.svg?invert_in_darkmode&sanitize=true" align=middle width=31.1511915pt height=16.438356pt/></p> 为用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p>的物品在 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/e5db46b49e39bd8b17d72b912e26bd71.svg?invert_in_darkmode&sanitize=true" align=middle width=12.608473349999999pt height=11.232861749999998pt/></p> 集合里面的排名， 矢量<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/8126c872b469423dfe8723df1d276805.svg?invert_in_darkmode&sanitize=true" align=middle width=17.167139549999998pt height=9.54335085pt/></p>表示用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p>的所有排名，排名顺序和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/31cb95ba4852441821ce8a4fe56120e8.svg?invert_in_darkmode&sanitize=true" align=middle width=31.1511915pt height=16.438356pt/></p> 的顺序一致。那么， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9538d4aeb3293f0392d17baee531b7d3.svg?invert_in_darkmode&sanitize=true" align=middle width=8.51598825pt height=11.232861749999998pt/></p> 可以通过下式估计：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/7fa5a890fbc9cad223ab31b44ee656e3.svg?invert_in_darkmode&sanitize=true" align=middle width=147.91131135pt height=39.452455349999994pt/></p>

其中， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9da5a9ab0a491b9a386e8007cabe3496.svg?invert_in_darkmode&sanitize=true" align=middle width=14.49208035pt height=13.698590399999999pt/></p> 为 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/4ad282d8cd57c018cffc817a60d07a9d.svg?invert_in_darkmode&sanitize=true" align=middle width=50.3651973pt height=12.6027363pt/></p> 的矩阵。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a75b3cb7cb1df56887dd548acad7cdd.svg?invert_in_darkmode&sanitize=true" align=middle width=12.067218899999999pt height=9.54335085pt/></p> 的估计也类似。

&ensp;&ensp;&ensp;&ensp;交替最小二乘法的优点就是比随机梯度下降法更加容易并行化，但是，它同时比随机梯度下降算法需要更大的内存。交替最小二乘法需要将全部数据集加载入内存里面，如果数据集 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a9bb7fa69ed54c7064249aa39a7009e2.svg?invert_in_darkmode&sanitize=true" align=middle width=14.06623185pt height=11.232861749999998pt/></p> 太大的话，可能会出现问题。

## 波兹曼限量机算法

&ensp;&ensp;&ensp;&ensp;除了低阶矩阵因子分解之外，还有很多其他的推荐算法。由于波兹曼限量机算法（RBMS）的逐渐流行，而且实际上，这种方法代表一种完全不同并具有很大竞争力的方法，所以在这部分，我们介绍下这种方法。波兹曼限量机实际上是一个两层带随机神经单元的神经网络。名字中的“限量”两词，是因为模型中的随机神经单元必须在一个二分图里面，如图9.2所示。

![&#x56FE;9.2  &#x5355;&#x7528;&#x6237;&#x7684;&#x6CE2;&#x5179;&#x66FC;&#x9650;&#x91CF;&#x673A;](../../.gitbook/assets/ac2a5c31-e5a0-4919-9967-e4f6713924cf.jpeg)

&ensp;&ensp;&ensp;&ensp;在波兹曼限量机的可见层神经单元必须和隐藏层神经单元相连，反之亦反之。层与层之间的连接是无向的，这意味着，波兹曼神经网络可以双向工作，隐藏层可以刺激可见层，同样，可见层也可以反过来刺激隐藏层。

&ensp;&ensp;&ensp;&ensp;在推荐设置中，建议为每个用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 构造一个单独的波兹曼限量机（RBM）。假设，用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 有 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/5d55430569d9a949e663776910069118.svg?invert_in_darkmode&sanitize=true" align=middle width=14.4331011pt height=7.0776222pt/></p> 个排名物品，那么在RBM网络中就有 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/5d55430569d9a949e663776910069118.svg?invert_in_darkmode&sanitize=true" align=middle width=14.4331011pt height=7.0776222pt/></p> 个相对应的可见单元点。

&ensp;&ensp;&ensp;&ensp;第 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/28b0b71e05b371ee11b28542314966d1.svg?invert_in_darkmode&sanitize=true" align=middle width=9.07536795pt height=11.4155283pt/></p> 个隐藏单元的输出通常是二元的，用 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/8991262a3fcaa628e1ac33e263416686.svg?invert_in_darkmode&sanitize=true" align=middle width=113.00413244999999pt height=16.438356pt/></p> 表示。第 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6ac91b4e7dd35551c6ea477deba5f82d.svg?invert_in_darkmode&sanitize=true" align=middle width=5.6632257pt height=10.84150485pt/></p> 个可见单元的输出用 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/878b013870fa95ea253444a858dc4f43.svg?invert_in_darkmode&sanitize=true" align=middle width=98.37718439999999pt height=16.438356pt/></p> 表示。在推荐系统中， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/4d8d47e91e5c3c8b08f6ce473b1faa65.svg?invert_in_darkmode&sanitize=true" align=middle width=12.6189657pt height=9.54335085pt/></p> 通常采用1到Q之间的有序离散值。<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/4d8d47e91e5c3c8b08f6ce473b1faa65.svg?invert_in_darkmode&sanitize=true" align=middle width=12.6189657pt height=9.54335085pt/></p> 中的第 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/5ea75157477d5d4abab688eea47348b8.svg?invert_in_darkmode&sanitize=true" align=middle width=7.92810645pt height=10.2739725pt/></p> 个值通常用 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/2c382cfe9c35de11be2b034a0ca8a39c.svg?invert_in_darkmode&sanitize=true" align=middle width=14.995680149999998pt height=17.4358833pt/></p>。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/2635954099bb71b8f35218239048ab35.svg?invert_in_darkmode&sanitize=true" align=middle width=14.995680149999998pt height=17.4358833pt/></p>有一定的几率被激活（波兹曼限量机网络会评估 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/4d8d47e91e5c3c8b08f6ce473b1faa65.svg?invert_in_darkmode&sanitize=true" align=middle width=12.6189657pt height=9.54335085pt/></p> 中的所有值的可能性分布\)。第 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/28b0b71e05b371ee11b28542314966d1.svg?invert_in_darkmode&sanitize=true" align=middle width=9.07536795pt height=11.4155283pt/></p> 个隐藏单元和第 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6ac91b4e7dd35551c6ea477deba5f82d.svg?invert_in_darkmode&sanitize=true" align=middle width=5.6632257pt height=10.84150485pt/></p> 个可见单元的第 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/5ea75157477d5d4abab688eea47348b8.svg?invert_in_darkmode&sanitize=true" align=middle width=7.92810645pt height=10.2739725pt/></p> 个值通过权重 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/11ceb9b6493d3d44a25e372d9d0c29d0.svg?invert_in_darkmode&sanitize=true" align=middle width=23.685436499999998pt height=17.81260305pt/></p> 相连。为避免混乱，误差并没有在图9.2中表示，对用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 的依赖也被忽略。

&ensp;&ensp;&ensp;&ensp;随机神经网络单元就是网络的输出依赖对网络输入的概率分布而不是一个确切的函数。隐藏单元是二元的，如下：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/1d50e9b96ab62c5c22611e2a06d5f698.svg?invert_in_darkmode&sanitize=true" align=middle width=258.8329854pt height=50.621184899999996pt/></p>

其中，

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/de055eaae61a0dd1610c2b86c242676c.svg?invert_in_darkmode&sanitize=true" align=middle width=45.74952525pt height=13.881256950000001pt/></p> 是误差，

 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/547232d8b6061c2f6218536a159a9a89.svg?invert_in_darkmode&sanitize=true" align=middle width=168.42637349999998pt height=18.0201615pt/></p> 是一个 sigmoid函数。

有序的可见神经单元通常遵循如下分类器规则：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6fcf2f6b95bb332b6a18db6accf8ebdb.svg?invert_in_darkmode&sanitize=true" align=middle width=304.94209185pt height=48.118737149999994pt/></p>

其中， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6c148dabe4d4a1b1e79ce506eb3a7688.svg?invert_in_darkmode&sanitize=true" align=middle width=13.492634099999998pt height=17.4358833pt/></p> 为方差。多个用户对同一个物品进行排名，所有的用户网络的共享连接权重和误差，但是每个用户的隐藏和可见的单元有不同的状态。

## 对比散度

&ensp;&ensp;&ensp;&ensp;波兹曼限量机（RBM）的参数是，权重和误差。权重和误差通过最大化可见神经单元的边缘可似性来获得，如下式:

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/8e993fee1800ba608d305ed49b78d8d2.svg?invert_in_darkmode&sanitize=true" align=middle width=252.67297770000002pt height=43.2211923pt/></p>

&ensp;&ensp;&ensp;&ensp;其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a7381118f3a04e7165d6ad3f45c7373.svg?invert_in_darkmode&sanitize=true" align=middle width=53.14690425pt height=16.438356pt/></p> 表示网络配置中的能量，通过下式表示：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/97077fab5c2a01bcb708b595fc04de9b.svg?invert_in_darkmode&sanitize=true" align=middle width=494.79936495pt height=50.621184899999996pt/></p>

其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6f28dbcc3b349f60abb04e391658fc5d.svg?invert_in_darkmode&sanitize=true" align=middle width=15.872379599999999pt height=13.698590399999999pt/></p>是归一化常量，学习是通过 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d59e8e399f55049faf5f0f6beaaa38f4.svg?invert_in_darkmode&sanitize=true" align=middle width=58.270599749999995pt height=16.438356pt/></p> 的梯度上升的方法来进行的。权重通过下式进行更新：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9d9c1d2e8280237c492a25d8e2876347.svg?invert_in_darkmode&sanitize=true" align=middle width=180.08509694999998pt height=39.6840972pt/></p>

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/5f4ebb8c2f8717197d4141f22ca48c8b.svg?invert_in_darkmode&sanitize=true" align=middle width=266.98184369999996pt height=39.6840972pt/></p>

​其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/8df31b234b06564ef0114a5a414cf7c7.svg?invert_in_darkmode&sanitize=true" align=middle width=17.3516574pt height=16.438356pt/></p> 表示期望。误差也做如上同样的更新。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c6e5127170e2874bbd87d2da75bdbad9.svg?invert_in_darkmode&sanitize=true" align=middle width=71.07015795pt height=17.55700485pt/></p> 等于，当网络通过数据集 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/a9bb7fa69ed54c7064249aa39a7009e2.svg?invert_in_darkmode&sanitize=true" align=middle width=14.06623185pt height=11.232861749999998pt/></p> 训练时，二元组 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0911630e2b4d649ab89e8db970da7edb.svg?invert_in_darkmode&sanitize=true" align=middle width=14.122013399999998pt height=13.881256950000001pt/></p> 和 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/2635954099bb71b8f35218239048ab35.svg?invert_in_darkmode&sanitize=true" align=middle width=14.995680149999998pt height=17.4358833pt/></p> 同时落在数据集 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/31f154638fd4f4dd976c3fbdcd5044df.svg?invert_in_darkmode&sanitize=true" align=middle width=14.06623185pt height=11.232861749999998pt/></p>的频率，表示网络中的可见单元 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/2635954099bb71b8f35218239048ab35.svg?invert_in_darkmode&sanitize=true" align=middle width=14.995680149999998pt height=17.4358833pt/></p> 是在数据集的数据之间。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a867ae9b0ff15f619c3be20cb6d547a.svg?invert_in_darkmode&sanitize=true" align=middle width=80.45776035pt height=17.55700485pt/></p> 表示学习模型定义的 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/2635954099bb71b8f35218239048ab35.svg?invert_in_darkmode&sanitize=true" align=middle width=14.995680149999998pt height=17.4358833pt/></p> 分布的期望。由于 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/0a867ae9b0ff15f619c3be20cb6d547a.svg?invert_in_darkmode&sanitize=true" align=middle width=80.45776035pt height=17.55700485pt/></p> 十分难计算，所以我们通常用近似值代替。利用蒙特卡洛对比散度方法，我们可以进行一下估计：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c9e78f7974ec7b4585ead9eabb1e8ee2.svg?invert_in_darkmode&sanitize=true" align=middle width=157.73150744999998pt height=17.55700485pt/></p>

其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/7a5b78f4da4b6d4b6a4932342a1e4638.svg?invert_in_darkmode&sanitize=true" align=middle width=54.534242400000004pt height=17.55700485pt/></p> 表示在 T 阶吉布斯抽样算法上的期望。

&ensp;&ensp;&ensp;&ensp;如我们所见，比如，在Netflix竞赛中，RBMS算法在矩阵因式分解比较困难的时候效果很好，反之亦然。正因为这样，一个成功的推荐算法通常同时结合利用矩阵因子分解推荐系统和RBM，来提供组合预测。

## 推荐系统的质量评估

&ensp;&ensp;&ensp;&ensp;评估一个推荐系统的最好标准依赖于实际中的推荐问题。在大多数的应用中，最小平方差（RMSE）通常是一个不错的选择。（最小平方差）RMSE 定义如下：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c0543a10a21592b34d8a84331ad0da8a.svg?invert_in_darkmode&sanitize=true" align=middle width=250.16762429999997pt height=59.17867724999999pt/></p>

它可以用来评估 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9c9a444254eb9c010e73403922cd6dab.svg?invert_in_darkmode&sanitize=true" align=middle width=21.818039099999996pt height=9.54335085pt/></p>真实值和预测值的数值上的差异。另外，RMSE还直接和等式1中的大多数矩阵优化方法中的待优化矩阵的数据拟合相关。这种关系和RMBS方法中的对数似然优化类似。

&ensp;&ensp;&ensp;&ensp;在有些场景下面，推荐系统生成的推荐榜单比生成单个特定的预测值 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9c9a444254eb9c010e73403922cd6dab.svg?invert_in_darkmode&sanitize=true" align=middle width=21.818039099999996pt height=9.54335085pt/></p> 显得更加重要。当我们可能仅仅需要的是一个排名前 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6aec6bc26afaa12bc00c3daffd500eb1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.872955199999999pt height=7.0776222pt/></p> 的榜单的时候，在这些场景中，我们可能更加倾向于使用信息检索的一个方法，比如说，在 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6aec6bc26afaa12bc00c3daffd500eb1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.872955199999999pt height=7.0776222pt/></p> 处的平均预测均值，一般用 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3822d2d4b539e2db989079b58296e4f.svg?invert_in_darkmode&sanitize=true" align=middle width=63.56367435pt height=11.4155283pt/></p>来表示 。对用户集合 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/366cb5a3a2d3ed5d07afd7e548cf1227.svg?invert_in_darkmode&sanitize=true" align=middle width=20.979918299999998pt height=13.698590399999999pt/></p> 定义如下：

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d0cdfa679508bd3be7038563787f783b.svg?invert_in_darkmode&sanitize=true" align=middle width=193.25859795pt height=47.7901512pt/></p>

<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/11df2ef74ba8f9624bdade4abf374c2d.svg?invert_in_darkmode&sanitize=true" align=middle width=278.40724065pt height=45.2741091pt/></p>

其中 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/33c03afea7e68da32d5d707cdbbfaaa1.svg?invert_in_darkmode&sanitize=true" align=middle width=52.1349015pt height=13.881256950000001pt/></p> 表示 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6aec6bc26afaa12bc00c3daffd500eb1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.872955199999999pt height=7.0776222pt/></p> 处用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 的平均准确率。<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/b5e9ce5990855716addbfb0267b84f32.svg?invert_in_darkmode&sanitize=true" align=middle width=47.0701539pt height=13.881256950000001pt/></p>表示<p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/28b0b71e05b371ee11b28542314966d1.svg?invert_in_darkmode&sanitize=true" align=middle width=9.07536795pt height=11.4155283pt/></p> 处的准确率，是与用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 相关的前 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/28b0b71e05b371ee11b28542314966d1.svg?invert_in_darkmode&sanitize=true" align=middle width=9.07536795pt height=11.4155283pt/></p>个结果中的一部分物品。变量 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3dfb52899aa08ff1acd8d5b3eb349dd.svg?invert_in_darkmode&sanitize=true" align=middle width=26.16836475pt height=15.936036599999998pt/></p> 表示物品和用户之间的相关性， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3dfb52899aa08ff1acd8d5b3eb349dd.svg?invert_in_darkmode&sanitize=true" align=middle width=26.16836475pt height=15.936036599999998pt/></p> 为1时，表示物品 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/28b0b71e05b371ee11b28542314966d1.svg?invert_in_darkmode&sanitize=true" align=middle width=9.07536795pt height=11.4155283pt/></p> 与用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 相关， <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3dfb52899aa08ff1acd8d5b3eb349dd.svg?invert_in_darkmode&sanitize=true" align=middle width=26.16836475pt height=15.936036599999998pt/></p> 为0时，表示物品 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/28b0b71e05b371ee11b28542314966d1.svg?invert_in_darkmode&sanitize=true" align=middle width=9.07536795pt height=11.4155283pt/></p> 与用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 不相关。 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/5cbf98dd801c0ce459e2e7bff9be7abe.svg?invert_in_darkmode&sanitize=true" align=middle width=81.89535255pt height=14.61184725pt/></p> 表示与用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 相关的物品总数。

&ensp;&ensp;&ensp;&ensp;由于 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3822d2d4b539e2db989079b58296e4f.svg?invert_in_darkmode&sanitize=true" align=middle width=63.56367435pt height=11.4155283pt/></p> 难以直接优化，所以我们通常用近似值和上界来代替。我们认为通过上界最小化值，和通过先优化RMSE，然后按照预测值 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/9c9a444254eb9c010e73403922cd6dab.svg?invert_in_darkmode&sanitize=true" align=middle width=21.818039099999996pt height=9.54335085pt/></p> 的降序排列对物品进行排列得到的最小化值相等。

## 推荐系统实战（R语言的SAS库）

&ensp;&ensp;&ensp;&ensp;R语言中，SAS的库中包含用户，书籍，书籍订阅，书籍续订的记录。作为一个应用示例，我们将利用低阶矩阵因子分解的方法来对给定用户是否会订阅或者续订某一本书。该数据集中有2212个用户和6763本书相关的8180条记录。续借次数在0和23之间，但是为了和只被借过一次的书籍和数据遗失的记录区分，我们对其做数据预处理，在原先的次数加上1，使得续借次数在1到24之间。

&ensp;&ensp;&ensp;&ensp;推荐系统的观察值为一个未知完全矩阵 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c1809668d64eff2bbf358a60787e123b.svg?invert_in_darkmode&sanitize=true" align=middle width=14.90868885pt height=11.232861749999998pt/></p> 的极小离散子集； <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/c1809668d64eff2bbf358a60787e123b.svg?invert_in_darkmode&sanitize=true" align=middle width=14.90868885pt height=11.232861749999998pt/></p> 中只有 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/8d5692e809c2be337304b5b9dd31eb87.svg?invert_in_darkmode&sanitize=true" align=middle width=152.65756274999998pt height=34.3600389pt/></p> 为观察值。

&ensp;&ensp;&ensp;&ensp;我们令 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/7bee7e1b473e6210d76c887d3846c092.svg?invert_in_darkmode&sanitize=true" align=middle width=53.49304949999999pt height=11.232861749999998pt/></p> 并在数据集中选择的随机子集上进行40次随机梯度下降训练。剩下的数据用来评估测试误差。该计算过程在一台标准的台式主机上只需花费数秒。结果的RMSE如图9.3所示。

&ensp;&ensp;&ensp;&ensp;与其尝试为一个特定的用户/书籍进行书籍续借的数量，这种本身来说没有用处的预测，为某个用户 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/20b7ff81f5270bcfeca598a09a3804e9.svg?invert_in_darkmode&sanitize=true" align=middle width=9.4102734pt height=7.0776222pt/></p> 推荐一个 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6aec6bc26afaa12bc00c3daffd500eb1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.872955199999999pt height=7.0776222pt/></p>本书的书单显得更加有趣。为了进行书单推荐，我们需要对所有的书籍 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6ac91b4e7dd35551c6ea477deba5f82d.svg?invert_in_darkmode&sanitize=true" align=middle width=5.6632257pt height=10.84150485pt/></p> 进行估算： <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6de8e503604cbea99b9399dfd39da206.svg?invert_in_darkmode&sanitize=true" align=middle width=77.01110505pt height=13.881256950000001pt/></p> ,然后对 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/27bf4a63454c510c70c70e3ceba5f504.svg?invert_in_darkmode&sanitize=true" align=middle width=21.818039099999996pt height=13.881256950000001pt/></p> 的列表降序排列。排名前 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/6aec6bc26afaa12bc00c3daffd500eb1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.872955199999999pt height=7.0776222pt/></p>就是被推荐的书籍 。值得注意的是，在上面的步骤中，我们进行 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3822d2d4b539e2db989079b58296e4f.svg?invert_in_darkmode&sanitize=true" align=middle width=63.56367435pt height=11.4155283pt/></p> 度量，因为 <p align="center"><img src="/translate/big-data-data-mining-and-machine-learning-value-creation-for-business-leaders-and-practitioners/tex/d3822d2d4b539e2db989079b58296e4f.svg?invert_in_darkmode&sanitize=true" align=middle width=63.56367435pt height=11.4155283pt/></p> 度量需要一个可靠的机制来确保每本被推荐的书籍都和每个用户相关。

![&#x56FE;9.3  &#x7559;&#x5B58;&#x6570;&#x636E;&#xFF08;&#x6765;&#x6E90;&#x4E8E;SAS &#xFF09;&#x7684;&#x5747;&#x65B9;&#x8BEF;&#x5DEE;](../../.gitbook/assets/339b8e1d-4585-4d32-b273-0ef390e1e780.jpeg)

&ensp;&ensp;&ensp;&ensp;作为一个具体的示例，我们在数据集中挑选一个特定的用户来检查推荐系统的效果。在训练中采用的随机数据子集，如下表9.2。最终由矩阵因子分解模型为该用户生成的10本推荐书籍，如下表9.3。该模型似乎捕捉到了用户对线性回归和经济学方面的主题的兴趣。

![](../../.gitbook/assets/2381aeaa-e3f0-443e-94cd-f15fe5f3968b.jpeg)

![&#x8868;9.2 &#x8BAD;&#x7EC3;&#x4E2D;&#x4F7F;&#x7528;&#x5230;&#x7684;&#x6570;&#x636E;&#x96C6;](../../.gitbook/assets/ac320d21-a00d-4e12-9564-147801541549.jpeg)

![&#x8868;9.3 &#x63A8;&#x8350;&#x7CFB;&#x7EDF;&#x4E3A;&#x7528;&#x6237;&#x63A8;&#x8350;&#x7684;10&#x672C;&#x4E66;&#x7C4D;](../../.gitbook/assets/7616c517-00f3-411f-b4ca-c4a4a0f2aab0.jpeg)



